# ğŸ“„ RAG Document Chatbot (FastAPI + Streamlit)
##  ğŸ“Œ Overview

A Retrieval-Augmented Generation (RAG) based document chatbot that allows users to upload documents (PDF/TXT) and chat with them using semantic search and an LLM.
The system performs background ingestion, vector storage, similarity search, and generates grounded answers strictly from the uploaded document.

---

## ğŸš€ Features

 - ğŸ“‚ Upload documents (.pdf, .txt)

- ğŸ§© Chunk documents and generate embeddings

- ğŸ§  Store embeddings in ChromaDB (local vector store)

- ğŸ” Semantic similarity search using Sentence Transformers

- ğŸ’¬ Chat with documents using Gemini LLM

- âš™ï¸ Background ingestion with status tracking

- â³ Real-time ingestion status polling

- ğŸš¦ Rate limiting for APIs

- ğŸ–¥ï¸ Clean Streamlit-based frontend


---
## ğŸ—ï¸ System Architecture
```
Frontend (Streamlit)
        |
        v
Backend (FastAPI)
  â”œâ”€â”€ Upload API
  â”œâ”€â”€ Background Ingestion
  â”œâ”€â”€ Status API
  â”œâ”€â”€ Chat API
        |
        v
Vector Store (ChromaDB)
        |
        v
LLM (Gemini 2.5 Flash)
```

---
## ğŸ› ï¸ Tech Stack

| Layer | Technologies |
| :--- | :--- |
| **Frontend** | Streamlit |
| **Backend** | FastAPI, Pydantic |
| **Document Processing** | PyMuPDF (PDF), Plain Text Parser |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) |
| **Vector Store** | ChromaDB |
| **LLM** | Google Gemini (`google-genai`) |
| **State Management** | In-memory ingestion state |
| **Rate Limiting** | SlowAPI |
| **Environment** | Python 3.10+, Virtualenv |

---

## ğŸ“ Project Structure
```text
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ documents.py
â”‚   â”‚       â”œâ”€â”€ chat.py
â”‚   â”‚       â””â”€â”€ health.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ vector_service.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ llm_service.py
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â””â”€â”€ ingestion/
â”‚   â”‚       â””â”€â”€ ingest_document.py
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â””â”€â”€ ingestion_state.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ app/db/
â”‚   â”œâ”€â”€ documents/
â”‚   â””â”€â”€ vector_store/chroma/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

frontend/
â””â”€â”€ app.py
```

---

## ğŸ”„ Application Flow
1ï¸âƒ£ **Document Upload**

- User uploads a PDF/TXT file via Streamlit

- Backend stores the file and assigns a document_id

- Ingestion starts in the background

2ï¸âƒ£ **Background Ingestion**

- Extract text from document

- Split text into chunks

- Generate embeddings

- Store chunks in ChromaDB

- Update ingestion status to completed

3ï¸âƒ£ **Status Polling**

- Frontend polls /documents/status/{document_id}

- Chat UI unlocks only after ingestion completes

4ï¸âƒ£ **Chat (RAG)**

- User query â†’ embedding

- Similarity search in ChromaDB

- Relevant chunks passed as context to Gemini

- Gemini generates answer only from document context


---

## ğŸ“Œ API Endpoints
**Upload Document**
```
POST /documents/upload
```

**Response**
```
{
  "document_id": "uuid",
  "status": "ingestion started"
}
```

**Check Ingestion Status**
```
GET /documents/status/{document_id}
```

**Response**
```
{
  "status": "processing | completed"
}
```

**Chat with Document**
```
POST /chat/
```

**Request**
```
{
  "document_id": "uuid",
  "query": "What does this document contain?"
}
```

**Response**
```
{
  "answer": "Answer generated strictly from the document"
}
```

---

## âš™ï¸ Setup Instructions
1ï¸âƒ£ **Clone the Repository**
```
git clone https://github.com/SistlaVishva04/RAG-Based-Question-Answering-System/tree/main/backend
cd backend
```

2ï¸âƒ£ **Create Virtual Environment**
```
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate      # Windows
```

3ï¸âƒ£ **Install Dependencies**
```
pip install -r requirements.txt
```

4ï¸âƒ£ **Environment Variables**
Create a .env file:
```
GEMINI_API_KEY=your_gemini_api_key
```

5ï¸âƒ£ **Run Backend (IMPORTANT)**
```
python -m uvicorn app.main:app
```

âš ï¸ Do NOT use **--reload**
(Stateful ingestion requires a single process)

6ï¸âƒ£ **Run Frontend**
```
streamlit run app.py
```

ğŸš¦ **Rate Limiting**

- Implemented using SlowAPI

- Protects backend endpoints from abuse


---

## ğŸ›¡ï¸ Safety & Hallucination Control

**The LLM is instructed**:
```
â€œAnswer ONLY using the provided context.
If the answer is not present, respond with:
I donâ€™t know based on the document.â€
```
This ensures **zero hallucination**.
**ğŸ§ª Example Output**

Query
```
What does this file contain?
```

Answer
```
This file contains information about the complexity of algorithms,
their properties, and an example algorithm with pseudocode.
```

âœ” Answer grounded in document content.

---

## ğŸ¯ Final Notes

This project demonstrates a production-style RAG pipeline with:

- Proper async ingestion

- Status tracking

- Vector-based retrieval

- LLM grounding

- Frontend-backend integration

---
## ğŸ‘¤ Author
Vishnu Vamsi

Email: vishnuvamsi04@gmail.com
